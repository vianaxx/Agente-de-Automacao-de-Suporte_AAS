Esse script em Python √© um **pipeline de Ingest√£o de Dados (cria√ß√£o do banco de dados)** para a cria√ß√£o de um **Retrieval-Augmented Generation (RAG) System** usando bibliotecas do ecossistema LangChain.

-----

## üõ†Ô∏è Vis√£o Geral do Mecanismo

O principal objetivo do script √© pegar documentos de texto (arquivos Markdown) e transform√°-los em uma representa√ß√£o num√©rica (*vetores* ou *embeddings*) para que possam ser facilmente pesquisados e recuperados por um modelo de Linguagem Grande (LLM) posteriormente.

Ele segue tr√™s etapas principais:

1.  **Carregar:** Ler os documentos brutos.
2.  **Dividir/Chunking:** Quebrar os documentos em peda√ßos menores.
3.  **Embed/Armazenar:** Converter os peda√ßos em vetores e salv√°-los em um banco de dados vetorial.

-----

## üîç Explica√ß√£o Detalhada do C√≥digo

### 1\. Configura√ß√µes Iniciais e Imports

```python
import os
from langchain_community.document_loaders import DirectoryLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

DATA_PATH = "data/docs"
CHROMA_PATH = "data/vector_db"

"""
Script para carregar os dados do data/docs/ no Vector DB. Execute este arquivo uma vez.
"""
```

* **Imports:** Importa as classes necess√°rias das bibliotecas LangChain (agora divididas em `langchain_community`, `langchain_huggingface`, `langchain_chroma`, etc.).
    * `os`: Para opera√ß√µes de sistema (cria√ß√£o de diret√≥rio).
    * `DirectoryLoader`: Classe para carregar documentos de um diret√≥rio.
    * `HuggingFaceEmbeddings`: Classe para gerar os vetores (embeddings) usando modelos do Hugging Face.
    * `Chroma`: O banco de dados vetorial escolhido para armazenar os vetores.
    * `RecursiveCharacterTextSplitter`: Uma estrat√©gia para dividir o texto.
* **Constantes (`DATA_PATH`, `CHROMA_PATH`):** Definem onde os documentos de entrada est√£o e onde o banco de dados vetorial ser√° salvo, respectivamente.

### 2\. Fun√ß√£o Principal de Ingest√£o (`ingest_data`)

#### A. Carregamento dos Documentos (Loading)

```python
def ingest_data():
    print("---1 - Carregando documentos ---")

    loader = DirectoryLoader(DATA_PATH, glob="**/*.md")
    documents = loader.load()
```

* **`DirectoryLoader`:** Inicializa um carregador que busca arquivos no `DATA_PATH` (i.e., `data/docs`).
* **`glob="**/*.md"`:** Isso √© um padr√£o de *wildcard* que instrui o carregador a buscar **todos** os arquivos com a extens√£o `.md` (Markdown) dentro do `DATA_PATH` e em quaisquer subdiret√≥rios (`**/*`).
* **`loader.load()`:** Executa o carregamento, retornando uma lista de objetos `Document` do LangChain. Cada objeto cont√©m o texto de um arquivo e seus metadados (como o caminho do arquivo original).

#### B. Divis√£o do Texto em Peda√ßos (Chunking)

```python
    # Breaks documents into smaller chunks (500 characters each with 50 character overlap)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    #The overlap ensures context isn't lost at chunk boundaries‚Äîeach chunk includes the last 50 characters of the previous chunk.
    texts = text_splitter.split_documents(documents)

    print(f"{len(texts)} documentos carregados")
```

* **Mecanismo:** Modelos de embedding e LLMs t√™m limites de entrada (tokens/caracteres). Al√©m disso, documentos grandes dificultam a pesquisa precisa. Por isso, os documentos s√£o divididos em "chunks".
* **`RecursiveCharacterTextSplitter`:** Tenta dividir o texto usando uma lista de separadores (`\n\n`, `\n`, `     `, etc.) em ordem decrescente de granularidade. Isso ajuda a manter a integridade sem√¢ntica, tentando n√£o quebrar frases ou par√°grafos no meio.
* **`chunk_size=500`:** O tamanho m√°ximo de cada peda√ßo de texto √© de **500 caracteres**.
* **`chunk_overlap=50`:** **Sobrelapagem** de 50 caracteres. Isso significa que os √∫ltimos 50 caracteres de um chunk s√£o tamb√©m os primeiros 50 caracteres do chunk seguinte.
    * **Por tr√°s do script:** A sobrelapagem √© crucial para garantir que o **contexto** n√£o seja perdido na fronteira entre os peda√ßos. Se uma ideia importante for dividida no limite de dois chunks, o overlap ajuda a garantir que o LLM receba informa√ß√µes contextuais completas ao buscar um dos peda√ßos.

#### C. Cria√ß√£o e Armazenamento dos Vetores (Embedding and Storing)

```python
    print("---2 - Criando embeddings com Hugging Face ---")

    # Use Hugging Face for embeddings (downloads model on first run)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # Store vectors in Chroma
    vector_store = Chroma.from_documents(
        texts,
        embeddings,
        persist_directory=CHROMA_PATH
    )
    print(f"Vetores armazenados em {CHROMA_PATH}")
    return vector_store
```

* **`HuggingFaceEmbeddings`:** Inicializa o modelo que far√° a convers√£o de texto para vetor.
* **`model_name="sentence-transformers/all-MiniLM-L6-v2"`:** Este √© um modelo de *Sentence Transformer* muito popular e eficiente.
    * **Por tr√°s do script:** O modelo pega cada peda√ßo de texto (chunk) e o transforma em uma lista longa de n√∫meros (*vetor*) em um espa√ßo dimensional (por exemplo, 384 dimens√µes para este modelo). Esse vetor captura o **significado sem√¢ntico** do texto. Textos com significados semelhantes ter√£o vetores que est√£o pr√≥ximos uns dos outros nesse espa√ßo.
* **`Chroma.from_documents(...)`:** Esta √© a etapa final e mais importante:
    * Pega a lista de **chunks de texto** (`texts`).
    * Usa o **modelo de embedding** (`embeddings`) para converter cada chunk em seu vetor correspondente.
    * Armazena o **vetor** e o **texto original** (e seus metadados) no banco de dados **Chroma** no caminho especificado (`CHROMA_PATH`).
* **`persist_directory=CHROMA_PATH`:** Garante que o banco de dados seja salvo no disco, permitindo que seja carregado novamente em outro script (o script de recupera√ß√£o/gera√ß√£o) sem a necessidade de reprocessar os documentos.

### 3\. Bloco de Execu√ß√£o Principal

```python
if __name__ == "__main__":
    # Garante que o diret√≥rio do DB exista
    os.makedirs(CHROMA_PATH, exist_ok=True)
    ingest_data()
```

* **`if __name__ == "__main__":`:** Garante que o c√≥digo dentro deste bloco s√≥ seja executado quando o script for rodado diretamente (e n√£o quando for importado como um m√≥dulo).
* **`os.makedirs(CHROMA_PATH, exist_ok=True)`:** Cria o diret√≥rio de destino (`data/vector_db`) se ele ainda n√£o existir.
* **`ingest_data()`:** Chama a fun√ß√£o que executa todo o pipeline de carregamento, divis√£o e armazenamento.

-----

## üí° Resumo do Fluxo de Recupera√ß√£o (RAG)

Quando voc√™ for usar esse DB em um script de resposta:

1.  O usu√°rio faz uma **pergunta** (ex: "Qual o processo X?").
2.  A **pergunta** tamb√©m √© transformada em um **vetor** usando o **MESMO** modelo de embedding.
3.  O banco de dados Chroma (`CHROMA_PATH`) √© consultado, buscando os **vetores** de documentos que s√£o **mais pr√≥ximos** do vetor da pergunta (usando c√°lculo de **similaridade de cosseno**).
4.  O script recupera os **chunks de texto** correspondentes aos vetores mais pr√≥ximos.
5.  Esses chunks de texto relevantes s√£o injetados como **contexto** na *prompt* enviada ao LLM (ex: "Baseado no contexto abaixo, responda √† pergunta: [Contexto: chunk 1, chunk 2, etc.] [Pergunta: Qual o processo X?]").
6.  O LLM gera uma resposta precisa, baseada na informa√ß√£o original dos seus documentos.
